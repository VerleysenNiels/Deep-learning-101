{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 101\n",
    "Deep learning is one of the most impactful technologies of the past few years and will continue to be of huge importance for our technological advancement. It's therefore important to have a good understanding of the basics before tackling more complex problems with existing libraries like Tensorflow or Pytorch. However, a lot of people still make wrong assumptions about what deep learning is or just see it as some kind of black magic. To help remedy this I wrote a series of blogs together with this repository. This notebook follows along with the blogs, explaining step by step what is going on.\n",
    "\n",
    "The blogs on medium:\n",
    "- Part 1 discusses the basics and the forward pass:  TBD\n",
    "- Part 2 discusses backpropagation:  TBD\n",
    "- Part 3 goes over several techniques to improve the learning ability of a network:  TBD\n",
    "\n",
    "This notebook handles the contents of the first blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make our lives a bit easier, I'll be using numpy \n",
    "import numpy as np\n",
    "\n",
    "# Later on we will also be creating some plots to visualize our results.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neuron\n",
    "A neuron is the most basic building block of a neural network. It receives an input and produces a single number as output from that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.884\n"
     ]
    }
   ],
   "source": [
    "# Let's say our input contains 5 numbers\n",
    "inputs = [4, 9, 2, 1.3, 5.6]\n",
    "\n",
    "# The neuron therefore needs to have 5 weights and of course a bias\n",
    "weights = [0.78, -0.34, 0.7, -0.2, -0.11]\n",
    "bias = 1.3\n",
    "\n",
    "# The output of the neuron can then be computed as follows:\n",
    "output = weights[0] * inputs[0] + weights[1] * inputs[1] + weights[2] * inputs[2] + weights[3] * inputs[3] + weights[4] * inputs[4] + bias\n",
    "\n",
    "# Resulting in:\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.884\n"
     ]
    }
   ],
   "source": [
    "# This is actually the same as doing a dot product of the input and the weights, and then adding the bias to it\n",
    "output = np.dot(weights, inputs) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "As you can see, the neuron is completely linear. So, how can neural networks learn nonlinear mappings between input and output spaces? The answer is using activation functions. Basically this comes down to passing the output of the neuron through a nonlinear function. There are many of these activation functions, one of the most widely used is the Rectified Linear Unit or ReLU for short. This activation function is quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ReLU returns 0 if x is smaller than 0, and x otherwise\n",
    "def ReLU(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# Or simply:    \n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.884\n"
     ]
    }
   ],
   "source": [
    "# Let's activate the output of the neuron\n",
    "output = ReLU(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "A neuron by itself can only do so much, therefore we combine them together to create neural networks. We structure this in layers, where each neuron of one layer receives the outputs from all neurons of the previous layer. By having multiple neurons in a single layer, they can focus themselves on different aspects of the mapping to be learned. While adding multiple layers together creates depth, meaning that later layers can base themselves on combinations of features found by earlier layers. We know that the forward pass of a single neuron is a dot product of the weights and the input matrices, to which we add the bias. This can easily be extended to account for a full layer. The weights matrix gets multiple rows with the weights of each neuron. The bias also has to be turned into a matrix, to be able to contain the bias of each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.884  8.932 11.042]\n"
     ]
    }
   ],
   "source": [
    "# We keep the same inputs as before\n",
    "inputs = [4, 9, 2, 1.3, 5.6]\n",
    "\n",
    "# Our layer has three neurons, so we need three sets of weights and three biases\n",
    "weights = [\n",
    "    [0.78, -0.34, 0.7, -0.2, -0.11],  # Same neuron as before\n",
    "    [-0.24, 1.24, 0.43, 0.04, -0.3],\n",
    "    [0.5, 0.65, -0.3, 0.64, 0.1]\n",
    "]\n",
    "\n",
    "biases = [1.3, -0.5, 2.4]\n",
    "\n",
    "# The forward pass remains the same\n",
    "output = np.dot(weights, inputs) + biases\n",
    "\n",
    "# The activation also remains the same, the numpy maximum functon can deal with vectors as well\n",
    "output = ReLU(output)\n",
    "\n",
    "# As a result we get three numbers, one for each neuron\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring our code\n",
    "Before continuing with the backpropagation algorithm, let's structure our code a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"Constructor of a single fully connected layer\n",
    "\n",
    "        Args:\n",
    "            input_size (int): number of inputs to this layer\n",
    "            output_size (int): number of neurons in this layer, which is equivalent to the number of outputs\n",
    "        \"\"\"\n",
    "        # Create our matrix with weights and biases for the layer\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.biases = np.zeros(output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the layer\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(self.inputs, self.weights.T) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    \n",
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the activation function\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, self.inputs)\n",
    "        return self.output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b03be9e07b1dc030a647a403c994d1f709ab3169abfdd5c15fecb89c5578e3a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
